openapi: 3.1.0
info:
  title: Ollama Generate API
  description: API for generating completions using the Ollama server.
  version: 0.1.0
servers:
  - url: http://localhost:11434
    description: Local Ollama server
paths:
  /api/generate:
    post:
      summary: Generate a completion
      description: |
        Generate a response for a given prompt with a provided model. This is a streaming endpoint by default, 
        returning a series of JSON objects. Streaming can be disabled with `"stream": false`. The final response 
        includes statistics and additional data.
      operationId: generateCompletion
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GenerateRequest'
            examples:
              streaming:
                summary: Streaming request
                value:
                  model: "llama3.2"
                  prompt: "Why is the sky blue?"
              nonStreaming:
                summary: Non-streaming request
                value:
                  model: "llama3.2"
                  prompt: "Why is the sky blue?"
                  stream: false
              withSuffix:
                summary: Request with suffix
                value:
                  model: "codellama:code"
                  prompt: "def compute_gcd(a, b):"
                  suffix: "    return result"
                  options:
                    temperature: 0
                  stream: false
              structuredOutput:
                summary: Structured output request
                value:
                  model: "llama3.1:8b"
                  prompt: "Ollama is 22 years old and is busy saving the world. Respond using JSON"
                  stream: false
                  format:
                    type: "object"
                    properties:
                      age:
                        type: "integer"
                      available:
                        type: "boolean"
                    required:
                      - "age"
                      - "available"
              jsonMode:
                summary: JSON mode request
                value:
                  model: "llama3.2"
                  prompt: "What color is the sky at different times of the day? Respond using JSON"
                  format: "json"
                  stream: false
              withImages:
                summary: Request with images
                value:
                  model: "llava"
                  prompt: "What is in this picture?"
                  stream: false
                  images:
                    - "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"
              rawMode:
                summary: Raw mode request
                value:
                  model: "mistral"
                  prompt: "[INST] why is the sky blue? [/INST]"
                  raw: true
                  stream: false
              reproducible:
                summary: Reproducible outputs request
                value:
                  model: "mistral"
                  prompt: "Why is the sky blue?"
                  options:
                    seed: 123
              withOptions:
                summary: Request with custom options
                value:
                  model: "llama3.2"
                  prompt: "Why is the sky blue?"
                  stream: false
                  options:
                    num_keep: 5
                    seed: 42
                    num_predict: 100
                    top_k: 20
                    top_p: 0.9
                    min_p: 0.0
                    typical_p: 0.7
                    repeat_last_n: 33
                    temperature: 0.8
                    repeat_penalty: 1.2
                    presence_penalty: 1.5
                    frequency_penalty: 1.0
                    mirostat: 1
                    mirostat_tau: 0.8
                    mirostat_eta: 0.6
                    penalize_newline: true
                    stop:
                      - "\n"
                      - "user:"
                    numa: false
                    num_ctx: 1024
                    num_batch: 2
                    num_gpu: 1
                    main_gpu: 0
                    low_vram: false
                    vocab_only: false
                    use_mmap: true
                    use_mlock: false
                    num_thread: 8
      responses:
        '200':
          description: |
            Success. Returns a stream of JSON objects if `stream` is true (default), or a single JSON object if `stream` is false.
          content:
            application/json:
              schema:
                oneOf:
                  - $ref: '#/components/schemas/GenerateStreamResponse'
                  - $ref: '#/components/schemas/GenerateSingleResponse'
              examples:
                streaming:
                  summary: Streaming response (partial)
                  value:
                    model: "llama3.2"
                    created_at: "2023-08-04T08:52:19.385406455-07:00"
                    response: "The"
                    done: false
                streamingFinal:
                  summary: Streaming final response
                  value:
                    model: "llama3.2"
                    created_at: "2023-08-04T19:22:45.499127Z"
                    response: ""
                    done: true
                    context: [1, 2, 3]
                    total_duration: 10706818083
                    load_duration: 6338219291
                    prompt_eval_count: 26
                    prompt_eval_duration: 130079000
                    eval_count: 259
                    eval_duration: 4232710000
                nonStreaming:
                  summary: Non-streaming response
                  value:
                    model: "llama3.2"
                    created_at: "2023-08-04T19:22:45.499127Z"
                    response: "The sky is blue because it is the color of the sky."
                    done: true
                    context: [1, 2, 3]
                    total_duration: 5043500667
                    load_duration: 5025959
                    prompt_eval_count: 26
                    prompt_eval_duration: 325953000
                    eval_count: 290
                    eval_duration: 4709213000
                withSuffix:
                  summary: Response with suffix
                  value:
                    model: "codellama:code"
                    created_at: "2024-07-22T20:47:51.147561Z"
                    response: "\n  if a == 0:\n    return b\n  else:\n    return compute_gcd(b % a, a)\n\ndef compute_lcm(a, b):\n  result = (a * b) / compute_gcd(a, b)\n"
                    done: true
                    done_reason: "stop"
                    context: [1, 2, 3]
                    total_duration: 1162761250
                    load_duration: 6683708
                    prompt_eval_count: 17
                    prompt_eval_duration: 201222000
                    eval_count: 63
                    eval_duration: 953997000
                structuredOutput:
                  summary: Structured output response
                  value:
                    model: "llama3.1:8b"
                    created_at: "2024-12-06T00:48:09.983619Z"
                    response: "{\n  \"age\": 22,\n  \"available\": true\n}"
                    done: true
                    done_reason: "stop"
                    context: [1, 2, 3]
                    total_duration: 1075509083
                    load_duration: 567678166
                    prompt_eval_count: 28
                    prompt_eval_duration: 236000000
                    eval_count: 16
                    eval_duration: 269000000
                jsonMode:
                  summary: JSON mode response
                  value:
                    model: "llama3.2"
                    created_at: "2023-11-09T21:07:55.186497Z"
                    response: "{\n\"morning\": {\n\"color\": \"blue\"\n},\n\"noon\": {\n\"color\": \"blue-gray\"\n},\n\"afternoon\": {\n\"color\": \"warm gray\"\n},\n\"evening\": {\n\"color\": \"orange\"\n}\n}\n"
                    done: true
                    context: [1, 2, 3]
                    total_duration: 4648158584
                    load_duration: 4071084
                    prompt_eval_count: 36
                    prompt_eval_duration: 439038000
                    eval_count: 180
                    eval_duration: 4196918000
                withImages:
                  summary: Response with images
                  value:
                    model: "llava"
                    created_at: "2023-11-03T15:36:02.583064Z"
                    response: "A happy cartoon character, which is cute and cheerful."
                    done: true
                    context: [1, 2, 3]
                    total_duration: 2938432250
                    load_duration: 2559292
                    prompt_eval_count: 1
                    prompt_eval_duration: 2195557000
                    eval_count: 44
                    eval_duration: 736432000
                rawMode:
                  summary: Raw mode response
                  value:
                    model: "mistral"
                    created_at: "2023-11-03T15:36:02.583064Z"
                    response: " The sky appears blue because of a phenomenon called Rayleigh scattering."
                    done: true
                    total_duration: 8493852375
                    load_duration: 6589624375
                    prompt_eval_count: 14
                    prompt_eval_duration: 119039000
                    eval_count: 110
                    eval_duration: 1779061000
                reproducible:
                  summary: Reproducible outputs response
                  value:
                    model: "mistral"
                    created_at: "2023-11-03T15:36:02.583064Z"
                    response: " The sky appears blue because of a phenomenon called Rayleigh scattering."
                    done: true
                    total_duration: 8493852375
                    load_duration: 6589624375
                    prompt_eval_count: 14
                    prompt_eval_duration: 119039000
                    eval_count: 110
                    eval_duration: 1779061000
                withOptions:
                  summary: Response with custom options
                  value:
                    model: "llama3.2"
                    created_at: "2023-08-04T19:22:45.499127Z"
                    response: "The sky is blue because it is the color of the sky."
                    done: true
                    context: [1, 2, 3]
                    total_duration: 4935886791
                    load_duration: 534986708
                    prompt_eval_count: 26
                    prompt_eval_duration: 107345000
                    eval_count: 237
                    eval_duration: 4289432000
components:
  schemas:
    GenerateRequest:
      type: object
      required:
        - model
      properties:
        model:
          type: string
          description: The model name in `model:tag` format (e.g., `llama3:70b`). Tag defaults to `latest` if omitted.
          example: "llama3.2"
        prompt:
          type: string
          description: The prompt to generate a response for.
          example: "Why is the sky blue?"
        suffix:
          type: string
          description: Text to append after the model response.
          example: "    return result"
        images:
          type: array
          items:
            type: string
            format: byte
          description: List of base64-encoded images (for multimodal models like `llava`).
        format:
          oneOf:
            - type: string
              enum: ["json"]
            - type: object
              description: JSON schema for structured outputs.
              additionalProperties: true
          description: |
            Format for the response. Can be `"json"` for JSON mode or a JSON schema for structured outputs.
          example: "json"
        options:
          $ref: '#/components/schemas/GenerateOptions'
        system:
          type: string
          description: System message to override the Modelfile definition.
        template:
          type: string
          description: Prompt template to override the Modelfile definition.
        stream:
          type: boolean
          description: If `false`, returns a single response object instead of a stream. Defaults to `true`.
          default: true
        raw:
          type: boolean
          description: If `true`, no formatting is applied to the prompt. No context is returned in this mode.
          default: false
        keep_alive:
          type: string
          description: Duration the model stays loaded in memory after the request (e.g., `5m`). In nanoseconds if numeric.
          default: "5m"
        context:
          type: array
          items:
            type: integer
          description: Context from a previous request for conversational memory (deprecated).
          deprecated: true
    GenerateOptions:
      type: object
      description: Additional model parameters.
      properties:
        num_keep:
          type: integer
          description: Number of tokens to keep.
        seed:
          type: integer
          description: Seed for reproducible outputs.
        num_predict:
          type: integer
          description: Maximum number of tokens to predict.
        top_k:
          type: integer
          description: Top-k sampling parameter.
        top_p:
          type: number
          description: Top-p (nucleus) sampling parameter.
        min_p:
          type: number
          description: Minimum probability threshold.
        typical_p:
          type: number
          description: Typical probability parameter.
        repeat_last_n:
          type: integer
          description: Number of last tokens to consider for repetition penalty.
        temperature:
          type: number
          description: Sampling temperature.
        repeat_penalty:
          type: number
          description: Penalty for repeated tokens.
        presence_penalty:
          type: number
          description: Penalty for presence of certain tokens.
        frequency_penalty:
          type: number
          description: Penalty based on token frequency.
        mirostat:
          type: integer
          description: Mirostat sampling mode.
        mirostat_tau:
          type: number
          description: Mirostat tau parameter.
        mirostat_eta:
          type: number
          description: Mirostat eta parameter.
        penalize_newline:
          type: boolean
          description: Whether to penalize newlines.
        stop:
          type: array
          items:
            type: string
          description: List of stop sequences.
        numa:
          type: boolean
          description: Enable NUMA optimizations.
        num_ctx:
          type: integer
          description: Context window size.
        num_batch:
          type: integer
          description: Batch size for processing.
        num_gpu:
          type: integer
          description: Number of GPUs to use.
        main_gpu:
          type: integer
          description: Main GPU index.
        low_vram:
          type: boolean
          description: Optimize for low VRAM usage.
        vocab_only:
          type: boolean
          description: Load only the vocabulary.
        use_mmap:
          type: boolean
          description: Use memory-mapped files.
        use_mlock:
          type: boolean
          description: Lock memory to prevent swapping.
        num_thread:
          type: integer
          description: Number of threads to use.
    GenerateStreamResponse:
      type: object
      description: A single object in the stream of responses when `stream` is `true`.
      required:
        - model
        - created_at
        - response
        - done
      properties:
        model:
          type: string
          description: The model name used for generation.
          example: "llama3.2"
        created_at:
          type: string
          format: date-time
          description: Timestamp of response creation.
          example: "2023-08-04T08:52:19.385406455-07:00"
        response:
          type: string
          description: Partial or empty response text. Empty in the final stream object.
          example: "The"
        done:
          type: boolean
          description: Indicates if this is the final response in the stream.
          example: false
        context:
          type: array
          items:
            type: integer
          description: Conversational context encoding (only in final response if not in raw mode).
        total_duration:
          type: integer
          format: int64
          description: Total time spent generating the response in nanoseconds (final response only).
          example: 10706818083
        load_duration:
          type: integer
          format: int64
          description: Time spent loading the model in nanoseconds (final response only).
          example: 6338219291
        prompt_eval_count:
          type: integer
          description: Number of tokens in the prompt (final response only).
          example: 26
        prompt_eval_duration:
          type: integer
          format: int64
          description: Time spent evaluating the prompt in nanoseconds (final response only).
          example: 130079000
        eval_count:
          type: integer
          description: Number of tokens in the response (final response only).
          example: 259
        eval_duration:
          type: integer
          format: int64
          description: Time spent generating the response in nanoseconds (final response only).
          example: 4232710000
    GenerateSingleResponse:
      type: object
      description: Response when `stream` is `false`.
      required:
        - model
        - created_at
        - response
        - done
      properties:
        model:
          type: string
          description: The model name used for generation.
          example: "llama3.2"
        created_at:
          type: string
          format: date-time
          description: Timestamp of response creation.
          example: "2023-08-04T19:22:45.499127Z"
        response:
          type: string
          description: The complete generated response.
          example: "The sky is blue because it is the color of the sky."
        done:
          type: boolean
          description: Always `true` for a single response.
          example: true
        done_reason:
          type: string
          description: Reason for completion (e.g., "stop").
          example: "stop"
        context:
          type: array
          items:
            type: integer
          description: Conversational context encoding (not returned in raw mode).
        total_duration:
          type: integer
          format: int64
          description: Total time spent generating the response in nanoseconds.
          example: 5043500667
        load_duration:
          type: integer
          format: int64
          description: Time spent loading the model in nanoseconds.
          example: 5025959
        prompt_eval_count:
          type: integer
          description: Number of tokens in the prompt.
          example: 26
        prompt_eval_duration:
          type: integer
          format: int64
          description: Time spent evaluating the prompt in nanoseconds.
          example: 325953000
        eval_count:
          type: integer
          description: Number of tokens in the response.
          example: 290
        eval_duration:
          type: integer
          format: int64
          description: Time spent generating the response in nanoseconds.
          example: 4709213000